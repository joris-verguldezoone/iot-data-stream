locust -f locust_producer.py
locust -f locust_consumer.py

# Faire Ã©voluer le sujet:
Si tempÃ©rature trop elevÃ©e, allumer un ventillateur 
Panne prÃ©vu 
Status d'utilisation (en cours ou non)
intÃ©raction api pour gÃ©rer l'IOT a distance
systeme a etat, peut etre faire du monitoring avec des alertes 

base de donnÃ©e = entrainement & apprentissage des stratÃ©gies a adopter
infÃ©rence = flux chaud pour mettre en pratique 
Bilan
Pannes
Facture d'electricitÃ© ? 
Optimisation gagnÃ© / productivitÃ© 
vitesse du ventillateur 

MCP

websocket ? 


20 cluster 
# sensor_type:
3 capteurs 
TempÃ©rature CPU
Consommation Ã©lectrique
DÃ©bit dâ€™air (airflow)

# Ventillateur
2 fan 
vitesse: 
faible moyen fort
Chaque vitesse a une consommation diffÃ©rente
ScÃ©narios de clustering (variantes)

Pour chaque scÃ©nario ci-dessous, hypothÃ¨se de base : chaque serveur = 3 capteurs (temp CPU, conso W, airflow) + 2 fans.

Petit cluster (5 serveurs)

Avantages : meilleure isolation, charge thermique plus facile Ã  dissiper, redondance simple.

InconvÃ©nients : moins de donnÃ©es pour dÃ©tecter patterns, une panne serveur = impact proportionnellement plus fort.

Test Ã  simuler : panne dâ€™un fan sur 1 serveur â†’ hausse locale de la tempÃ©rature; lâ€™IA devrait dÃ©tecter la dÃ©viation rapide et proposer action (monter speed ou basculer en manuel).

Cluster moyen (10 serveurs)

Avantages : agrÃ©gation utile pour corrÃ©lation, possibilitÃ© dâ€™Ã©quilibrer charge.

InconvÃ©nients : points chauds localisÃ©s peuvent rester invisibles si on ne regarde que la moyenne.

Test : hausse modÃ©rÃ©e de charge CPU sur 2 serveurs â†’ conso â†‘ + temp â†‘; si airflow baisse sur 1 serveur (capteur airflow dÃ©faillant), lâ€™IA doit distinguer Â«capteur mortÂ» vs Â«vrai problÃ¨meÂ».

Cluster large (15 serveurs)

Avantages : plus de donnÃ©es â†’ meilleurs modÃ¨les ML, meilleure robustesse statistique.

InconvÃ©nients : propagation thermique possible (hot-aisle effects) ; consommation cumulÃ©e Ã©levÃ©e.

Test : simuler dÃ©faillance partielle de la clim de la salle â†’ tempÃ©ratures globales montent progressivement sur plusieurs serveurs ; lâ€™IA devrait dÃ©tecter corrÃ©lation spatiale (plusieurs serveurs du mÃªme rack / cluster montent ensemble).

TrÃ¨s large (20 serveurs) â€” scÃ©nario par dÃ©faut

Avantages : forte volumÃ©trie pour entrainer modÃ¨les, possibilitÃ© dâ€™optimisation Ã©nergÃ©tique Ã  lâ€™Ã©chelle du cluster.

InconvÃ©nients : plus dâ€™interactions non-linÃ©aires (ventilos, flux dâ€™air, PDU) â†’ politiques de contrÃ´le plus complexes.

Test : montÃ©e de charge transitoire (pic), + 1 ventilateur tombe progressivement (dÃ©gradation) â†’ lâ€™IA doit prÃ©dire la panne totale du fan et planifier remplacement / redistribution de charge.

Causes de pannes simulÃ©es (Ã  injecter)

DÃ©faillance matÃ©rielle soudaine : fan OFF, capteur qui renvoie NULL, disque qui saute.

DÃ©gradation progressive : fan qui perd 1-2% dâ€™efficacitÃ© toutes les heures (courbe linÃ©aire ou exponentielle).

Erreur de capteur : drift systÃ©matique (offset) ou stuck value.

Surcharge logicielle : processus CPU intensif simulÃ© â†’ temp + conso â†‘.

ProblÃ¨me dâ€™infrastructure : PDU (alim) qui limite la puissance, climatisation qui baisse en performance.

InterfÃ©rence rÃ©seau : perte de tÃ©lÃ©metrie intermittente (paquets manquants), entraÃ®nant trous dans sÃ©ries temporelles.

Effets de regroupement : hot-aisle / cold-aisle â€” si le flux dâ€™air est mal rÃ©parti, serveurs voisins impactÃ©s.

MÃ©triques & KPIs Ã  observer

TempÃ©rature (Â°C) : moyenne, max par serveur, dÃ©rivÃ©e (delta/min).

Consommation (W) : par serveur / cluster, pics, Ã©nergie cumulative (kWh).

Airflow (mÂ³/s ou valeur relative) : chute absolue ou relative.

Fans : status (ON/OFF), speed_percent, temps de rÃ©ponse.

DisponibilitÃ© capteurs (missing %).

Anomalies dÃ©tectÃ©es (count/jour), FA/FR rates (faux positifs/nÃ©gatifs).

Latence de dÃ©tection et temps moyen pour remÃ©diation (simulÃ©).

ExpÃ©riences concrÃ¨tes Ã  lancer (protocoles)

Pour chaque taille de cluster, exÃ©cute ces tests et collecte rÃ©sultats :

Test â€œfan failâ€ (soudain)

Injecter fan.status = OFF sur un fan dâ€™un serveur Ã  t0.

Observables : temp_local â†‘ (p.ex. +6â€“12Â°C), fans restants augmentent speed, consommation inchangÃ©e.

VÃ©rifie si lâ€™IA propose : augmenter speed du fan adjacent, migrer jobs, alerter maintenance.

Test â€œsensor driftâ€ (progressif)

Appliquer offset +0.5Â°C par heure sur un capteur.

Observables : divergence entre capteurs du mÃªme serveur â†’ signe dâ€™erreur capteur.

Lâ€™IA doit apprendre Ã  corrÃ©ler capteurs (ex. tempÃ©rature CPU vs chassis) pour dÃ©tecter drift.

Test â€œclim dÃ©gradÃ©eâ€ (cluster-wide)

Baisser airflow ambiant ou la capacitÃ© de la clim (simulateur) â†’ temps: montÃ©e lente sur plusieurs serveurs.

Observables : pattern spatial (serveurs en bas du rack plus chauds).

Lâ€™IA doit classifier Â«issue cluster vs serverÂ».

Test â€œcharge burstâ€

Simuler pic de CPU sur N serveurs (1, 5, 10 selon taille).

Observables : conso â†‘, temp â†‘ ; si N grand â†’ tempÃ©rature cluster augmente plus, puis fans saturent.

Ã‰tudier seuils oÃ¹ actions prÃ©ventives deviennent nÃ©cessaires.

Test â€œpartial network lossâ€

Simuler perte de tÃ©lÃ©mÃ©trie 10â€“30s sur certains capteurs.

Observables : trous temporels ; lâ€™IA/ingestion doit interpoler ou marquer donnÃ©es manquantes.

HypothÃ¨ses / apprentissages que lâ€™IA peut tirer

CorrÃ©lations multi-capteur : temp â†‘ + conso â†‘ â†’ vraie charge ; temp â†‘ seule â†’ airflow/ventilateur dÃ©fectueux ou capteur.

DÃ©tection prÃ©coce : dÃ©gradation progressive des fans peut Ãªtre identifiÃ©e par augmentation progressive de speed_percent et temps de rÃ©ponse.

Classification spatiale : pannes cluster-wide vs server-level grÃ¢ce Ã  motifs spatiaux (plusieurs serveurs identiques).

Optimisation Ã©nergÃ©tique : apprendre politiques pour limiter consommation globale (par ex. baisser speed sur serveurs dÃ©jÃ  froids et augmenter juste oÃ¹ nÃ©cessaire).

Comment injecter les scÃ©narios (pratique)

Seeder : script Python qui crÃ©e clusters/servers/sensors/fans (tu lâ€™as dÃ©jÃ  en tÃªte).

Fault injector : module qui, selon calendrier, modifie en DB les lignes fan.status, sensor.last_value ou injecte valeurs corrompues dans sensor_data.

Orchestrateur : scheduler (cron ou APScheduler) qui lance les tests (on/off, drift, bursts).

Observability : dashboard (Grafana) + alerting (Prometheus alertmanager ou simple webhook) pour voir effets en temps rÃ©el.

Logging : stocker Ã©vÃ©nements de fault injection pour labels ML (anomaly = true).

Exemples de paramÃ¨tres numÃ©riques (Ã  copier-coller)

DÃ©gradation de fan : speed_percent -= 2 tous les 30 min jusquâ€™Ã  speed_percent <= 40 puis status = OFF.

Drift capteur : value += 0.2 Â°C / heure.

Charge burst : duration=120s, CPU load = 90% sur N serveurs.

Pertes tÃ©lÃ©mÃ©trie : drop messages MQTT pendant 10â€“30s sur 20% des capteurs.

# Mesure cout electrique 
Mesures de consommation (Watt) â†’ dÃ©jÃ  prÃ©vues avec tes capteurs POWER.

DurÃ©e de fonctionnement â†’ chaque mesure sensor_data est timestampÃ©e.

Prix du kWh â†’ paramÃ¨tre configurable (ex. 0,20 â‚¬/kWh en France, variable selon contrat).

## Exemple 
Exemple concret

1 serveur consomme en moyenne 200 W.

Cluster = 20 serveurs â†’ 200 W Ã— 20 = 4000 W = 4 kW.

Si cluster tourne 24h â†’

4
â€‰
kW
Ã—
24
â€‰
h
=
96
â€‰
kWh
4kWÃ—24h=96kWh

Prix de lâ€™Ã©lectricitÃ© = 0,20 â‚¬/kWh â†’

96 Ã— 0.20 = 19.20 â‚¬ /ğ‘—ğ‘œğ‘¢ğ‘Ÿ

96Ã—0.20=19.20â‚¬/jour

Donc un cluster de 20 serveurs consomme environ ~20 â‚¬ par jour.

Charge dynamique : la consommation varie avec lâ€™utilisation CPU â†’ il faut intÃ©grer les donnÃ©es capteurs temporelles.

Ventilateurs : chaque fan consomme aussi (typiquement 2â€“10 W chacun).

Overhead (climatisation, PDU, UPS) â†’ introduire un PUE (Power Usage Effectiveness).

Exemple : si PUE = 1,5 â†’ chaque 1 kWh IT entraÃ®ne 0,5 kWh infra.

CoÃ»t rÃ©el = CoutÂ ITÃ— ğ‘ƒğ‘ˆğ¸ coutÂ ITÃ—PUE.

# Le batching 

Ce quâ€™on peut calculer dans chaque batch

Pour chaque capteur et chaque fenÃªtre (par ex. 5 min), tu peux calculer :

avg = moyenne (ex : conso moyenne 5 min)

min = valeur minimale

max = valeur maximale

first = premiÃ¨re valeur de la fenÃªtre

last = derniÃ¨re valeur de la fenÃªtre

count = nombre de points

Candlestick = oui, tu peux !

Les candlesticks viennent du monde financier (OHLC : Open, High, Low, Close), mais Ã§a marche aussi pour lâ€™IoT :

Open â†’ premiÃ¨re mesure de la fenÃªtre

High â†’ max de la fenÃªtre

Low â†’ min de la fenÃªtre

Close â†’ derniÃ¨re mesure de la fenÃªtre

Donc si tu fais du batch en sÃ©ries temporelles, tu peux produire directement des chandeliers (candlestick charts) pour :

tempÃ©rature CPU dâ€™un serveur,

consommation Ã©lectrique dâ€™un cluster,

airflow dâ€™un ventilateur.

Et tu peux tracer Ã§a avec nâ€™importe quel outil (Grafana, matplotlib, plotlyâ€¦).


Capteurs & serveurs

Temp sensors par serveur : 2â€“4 (CPU, inlet, chassis).

Power sensor : 1 par serveur (ou 1 PDU per rack + estimation per server).

Airflow sensor : 0â€“1 par serveur + 1 par rack.

Fan : 2 par serveur OK, mais simuler aussi fans rack/room.

Valeurs typiques (ordres de grandeur)

Serveur idle â‰ˆ 50â€“120 W (selon type), peak â‰ˆ 150â€“500 W (serveur lourd / GPU beaucoup plus).

Fan per server â‰ˆ 2â€“20 W chacun (dÃ©pendant du modÃ¨le).

PUE datacenter rÃ©aliste : 1.1â€“1.8 (1.2 bon).

FrÃ©quence dâ€™Ã©chantillonnage conseillÃ©e

TempÃ©rature CPU / airflow : 5â€“15 s pour dÃ©tection rapide des pics.

Puissance (W) : 15â€“60 s (les PDU peuvent Ãªtre plus lents).

Ã‰tats fans / status : on/off en temps rÃ©el, log quand changement.

Historique Ã  garder haute frÃ©quence â†’ downsample pour stockage long terme (5s â†’ 1m â†’ 1h).

ModÃ©lisation des pannes

DÃ©faillance soudaine : injecter status = OFF Ã  t0 (probabilitÃ© p par pÃ©riode).

DÃ©gradation : fan efficiency -= x% every Y hours (simulate wear).

Sensor drift : add bias drawn from normal distro (mu=0, sigma small) cumulative.

Network loss : drop rate 0â€“5% intermittently.

Statistiques & distributions

MTBF / temps de panne : modÃ©liser avec Weibull ou exponentielle pour la probabilitÃ© dâ€™Ã©chec.

Charge bursts : utiliser Poisson process pour arrivÃ©es de jobs/pics.


Garder ton modÃ¨le actuel comme baseline (20/10/5 serveurs par cluster).

Augmenter capteurs par serveur : remplacer 1 temp par 3 sondes (cpu/inlet/chassis).

Ajouter profil de conso : idle/baseline/peak par serveur, gÃ©nÃ©rÃ©s stochastiquement.

Ajouter fans rack / PUE dans le calcul de coÃ»t.

DÃ©finir rÃ¨gles de panne (soudaines, drift, dÃ©gradation) avec paramÃ¨tres simples (probabilitÃ©, amplitude, durÃ©e).

Choix dâ€™Ã©chantillonnage : temp 10s, power 30s, stockage long terme downsample 1m.

Si tu veux, je peux gÃ©nÃ©rer directement :

un script seed qui crÃ©e les capteurs supplÃ©mentaires, profils de conso et PUE ; ou

un module de simulation / fault injector qui applique les distributions (Weibull, drift, bursts) et produit des sÃ©ries temporelles rÃ©alistes.